# CUDA 进阶练习题集

> 本练习题集针对已掌握基础 CUDA 编程的学习者，覆盖共享内存、原子操作、Streams、性能优化等高级主题。

---

## 📚 使用说明

- **前置要求**：完成基础教程 (01-04) 和基础练习题集
- **难度等级**：⭐⭐⭐ 到 ⭐⭐⭐⭐⭐
- **建议顺序**：按章节顺序完成

---

## 🔥 第一部分：共享内存

### 练习 2.1：共享内存数组求和

**难度**: ⭐⭐⭐

**目标**：使用共享内存实现线程块内的数组求和（规约）。

**要求**：
- 输入：长度为 N 的数组
- 每个线程块使用共享内存进行局部规约
- 最终在主机端合并各块的结果

**提示**：
```cuda
__shared__ float sdata[256];
// 加载数据到共享内存
sdata[tid] = input[globalIdx];
__syncthreads();

// 规约
for (int s = blockDim.x / 2; s > 0; s >>= 1) {
    if (tid < s) {
        sdata[tid] += sdata[tid + s];
    }
    __syncthreads();
}
```

---

### 练习 2.2：矩阵转置优化

**难度**: ⭐⭐⭐⭐

**目标**：实现三个版本的矩阵转置并比较性能。

**要求**：
1. **基础版本**：直接全局内存访问
2. **共享内存版本**：使用 tile 减少非合并访问
3. **无 bank conflict 版本**：使用 padding

**测试矩阵**：1024 × 1024

**预期结果**：
- 基础版本：~X ms
- 共享内存：~Y ms（应该更快）
- 无 bank conflict：~Z ms（最快）

**骨架代码**：
```cuda
#define TILE_DIM 32

__global__ void transposeShared(float *out, const float *in, int width) {
    __shared__ float tile[TILE_DIM][TILE_DIM + 1]; // +1 避免 bank conflict

    int x = blockIdx.x * TILE_DIM + threadIdx.x;
    int y = blockIdx.y * TILE_DIM + threadIdx.y;

    // 读取到共享内存
    if (x < width && y < width) {
        tile[threadIdx.y][threadIdx.x] = in[y * width + x];
    }
    __syncthreads();

    // 计算转置后的坐标
    x = blockIdx.y * TILE_DIM + threadIdx.x;
    y = blockIdx.x * TILE_DIM + threadIdx.y;

    // 写回全局内存
    if (x < width && y < width) {
        out[y * width + x] = tile[threadIdx.x][threadIdx.y];
    }
}
```

---

### 练习 2.3：卷积优化

**难度**: ⭐⭐⭐⭐

**目标**：使用共享内存优化 2D 卷积。

**要求**：
- 实现 5×5 高斯滤波
- 将卷积核存储在常量内存
- 使用共享内存缓存输入数据（包含 halo 区域）

**提示**：
```cuda
__constant__ float d_kernel[5][5];

__global__ void conv2DShared(float *out, const float *in,
                              int width, int height) {
    __shared__ float smem[TILE_SIZE + 4][TILE_SIZE + 4];

    // 加载 tile + halo 到共享内存
    // ...

    __syncthreads();

    // 使用共享内存进行卷积
    // ...
}
```

---

## ⚡ 第二部分：原子操作与同步

### 练习 3.1：直方图统计

**难度**: ⭐⭐⭐

**目标**：实现 GPU 加速的灰度图像直方图统计。

**要求**：
1. 基础版本：全局内存原子操作
2. 优化版本：共享内存局部直方图 + 合并

**输入**：灰度图像（uint8, 0-255）
**输出**：256 个 bin 的直方图

**对比**：测量两个版本的性能差异

---

### 练习 3.2：并行前缀和 (Scan)

**难度**: ⭐⭐⭐⭐

**目标**：实现 Blelloch 算法的并行前缀和。

**要求**：
- 输入：长度为 N 的数组
- 输出：exclusive scan 结果
- 处理任意长度（需要多级 scan）

**算法概述**：
```
Up-sweep (规约):
[3, 1, 7, 0, 4, 1, 6, 3]
→ [3, 4, 7, 7, 4, 5, 6, 9]
→ [3, 4, 7, 11, 4, 5, 6, 14]
→ [3, 4, 7, 11, 4, 5, 6, 25]

Down-sweep:
[3, 4, 7, 11, 4, 5, 6, 0]  // 设置最后为0
→ [3, 4, 7, 0, 4, 5, 6, 11]
→ [3, 0, 7, 4, 4, 11, 6, 16]
→ [0, 3, 4, 11, 11, 15, 16, 22]
```

---

### 练习 3.3：自旋锁实现

**难度**: ⭐⭐⭐⭐⭐

**目标**：使用原子操作实现简单的自旋锁。

**要求**：
- 实现 `lock()` 和 `unlock()` 函数
- 使用 `atomicCAS` 实现
- 测试多线程竞争场景

**骨架代码**：
```cuda
__device__ void lock(int *mutex) {
    while (atomicCAS(mutex, 0, 1) != 0) {
        // 自旋等待
    }
}

__device__ void unlock(int *mutex) {
    atomicExch(mutex, 0);
}
```

**思考题**：为什么在 GPU 上使用锁通常不是好主意？

---

## 🌊 第三部分：CUDA Streams

### 练习 4.1：流水线数据处理

**难度**: ⭐⭐⭐

**目标**：使用多个 Stream 实现计算与传输重叠。

**任务**：
- 处理 4 个数据块
- 每个块：传输到 GPU → 计算 → 传输回 CPU
- 使用 4 个 Stream 并行化

**测量**：
- 串行执行时间
- 流水线执行时间
- 加速比

**骨架代码**：
```cuda
cudaStream_t streams[4];
for (int i = 0; i < 4; i++) {
    cudaStreamCreate(&streams[i]);
}

for (int i = 0; i < 4; i++) {
    cudaMemcpyAsync(d_data[i], h_data[i], size,
                    cudaMemcpyHostToDevice, streams[i]);
    kernel<<<grid, block, 0, streams[i]>>>(d_data[i], N);
    cudaMemcpyAsync(h_result[i], d_data[i], size,
                    cudaMemcpyDeviceToHost, streams[i]);
}

cudaDeviceSynchronize();
```

---

### 练习 4.2：事件计时

**难度**: ⭐⭐⭐

**目标**：精确测量内核执行时间和内存传输时间。

**要求**：
- 使用 `cudaEvent_t` 分别测量：
  1. H→D 传输时间
  2. 内核执行时间
  3. D→H 传输时间
- 计算各部分占比

---

### 练习 4.3：回调函数

**难度**: ⭐⭐⭐⭐

**目标**：使用 Stream 回调实现异步通知。

**要求**：
- 使用 `cudaStreamAddCallback`
- 当特定 Stream 操作完成时打印消息
- 实现简单的任务完成统计

---

## 🎯 第四部分：性能优化

### 练习 5.1：内存合并分析

**难度**: ⭐⭐⭐

**目标**：理解和优化内存访问模式。

**任务**：比较以下访问模式的性能：
1. 合并访问：`data[tid]`
2. 跨步访问：`data[tid * stride]`
3. 随机访问：`data[indices[tid]]`

**测量**：使用 Nsight Compute 分析内存效率

---

### 练习 5.2：占用率优化

**难度**: ⭐⭐⭐⭐

**目标**：通过调整参数最大化 SM 占用率。

**任务**：
1. 编写一个使用大量寄存器的内核
2. 使用 `cudaOccupancyMaxPotentialBlockSize` 自动选择块大小
3. 使用 `__launch_bounds__` 手动限制
4. 比较不同配置下的性能

**工具**：
```cuda
int minGridSize, blockSize;
cudaOccupancyMaxPotentialBlockSize(&minGridSize, &blockSize,
                                    myKernel, 0, 0);
```

---

### 练习 5.3：分支发散优化

**难度**: ⭐⭐⭐⭐

**目标**：消除或减少分支发散。

**原始代码**（有分支发散）：
```cuda
__global__ void processData(float *data, int n) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid < n) {
        if (data[tid] > 0) {
            data[tid] = sqrtf(data[tid]);
        } else {
            data[tid] = 0;
        }
    }
}
```

**任务**：
1. 分析分支发散的影响
2. 重写代码消除分支发散
3. 比较性能

**提示**：使用数学函数替代条件判断
```cuda
data[tid] = fmaxf(0.0f, data[tid]);
data[tid] = sqrtf(data[tid]);
```

---

## 🔬 第五部分：算法实现

### 练习 6.1：并行归并排序

**难度**: ⭐⭐⭐⭐

**目标**：实现 GPU 并行归并排序。

**要求**：
- 块内使用共享内存排序
- 块间使用全局内存归并
- 与 `thrust::sort` 比较性能

---

### 练习 6.2：稀疏矩阵向量乘法

**难度**: ⭐⭐⭐⭐

**目标**：实现 CSR 格式的 SpMV。

**CSR 格式**：
- `values[]`：非零值
- `colIndices[]`：列索引
- `rowPtrs[]`：行指针

**骨架代码**：
```cuda
__global__ void spmv_csr(float *y, const float *values,
                          const int *colIndices, const int *rowPtrs,
                          const float *x, int numRows) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    if (row < numRows) {
        float sum = 0.0f;
        for (int j = rowPtrs[row]; j < rowPtrs[row + 1]; j++) {
            sum += values[j] * x[colIndices[j]];
        }
        y[row] = sum;
    }
}
```

**优化**：尝试使用向量化加载和共享内存

---

### 练习 6.3：K-Means 聚类

**难度**: ⭐⭐⭐⭐⭐

**目标**：实现完整的 GPU 加速 K-Means 算法。

**步骤**：
1. **分配阶段**：每个点找最近中心（并行）
2. **更新阶段**：计算新中心（规约）

**挑战**：
- 高效的距离计算
- 并行规约求新中心
- 处理空簇

---

## 🎮 第六部分：综合项目

### 练习 7.1：实时图像滤波器

**难度**: ⭐⭐⭐⭐

**目标**：实现一个支持多种滤波器的图像处理管道。

**功能**：
- 高斯模糊
- 锐化
- 边缘检测 (Sobel)
- 亮度/对比度调整

**要求**：
- 使用共享内存优化卷积
- 使用常量内存存储滤波器核
- 支持任意尺寸图像

---

### 练习 7.2：N 体模拟

**难度**: ⭐⭐⭐⭐⭐

**目标**：实现 GPU 加速的 N 体引力模拟。

**要求**：
1. 基础 O(N²) 实现
2. 共享内存分块优化
3. 可视化输出（可选）

**优化目标**：16384 个粒子达到实时（>30 fps）

---

### 练习 7.3：神经网络前向传播

**难度**: ⭐⭐⭐⭐⭐

**目标**：从零实现简单 MLP 的 GPU 前向传播。

**网络结构**：
- 输入层：784（MNIST）
- 隐藏层：256（ReLU）
- 输出层：10（Softmax）

**实现**：
- 矩阵乘法（全连接层）
- ReLU 激活
- Softmax 输出

---

## 📊 评分标准

| 练习 | 正确性 | 性能 | 代码质量 |
|------|--------|------|----------|
| 基础 (⭐⭐⭐) | 60% | 20% | 20% |
| 进阶 (⭐⭐⭐⭐) | 50% | 30% | 20% |
| 挑战 (⭐⭐⭐⭐⭐) | 40% | 40% | 20% |

---

## 💡 调试技巧

1. **使用 `printf` 调试**
   ```cuda
   if (threadIdx.x == 0 && blockIdx.x == 0) {
       printf("Debug: value = %f\n", someValue);
   }
   ```

2. **检查内核错误**
   ```cuda
   kernel<<<grid, block>>>(...);
   cudaError_t err = cudaGetLastError();
   if (err != cudaSuccess) {
       printf("Kernel error: %s\n", cudaGetErrorString(err));
   }
   ```

3. **使用 compute-sanitizer**
   ```bash
   compute-sanitizer ./my_program
   ```

4. **使用 Nsight Compute 分析**
   ```bash
   ncu --set full ./my_program
   ```

---

## 🎯 完成检查清单

- [ ] 第一部分：共享内存（3 题）
- [ ] 第二部分：原子操作（3 题）
- [ ] 第三部分：Streams（3 题）
- [ ] 第四部分：性能优化（3 题）
- [ ] 第五部分：算法实现（3 题）
- [ ] 第六部分：综合项目（3 题）

**总计**：18 道进阶练习题

祝你学习顺利！🚀
