# CUDA 性能优化原理

> 深入理解 GPU 性能优化的底层原理，掌握内存访问、指令调度、并行度优化等核心技术。

## 目录

1. [性能分析基础](#1-性能分析基础)
2. [内存访问优化](#2-内存访问优化)
3. [共享内存优化](#3-共享内存优化)
4. [指令级优化](#4-指令级优化)
5. [占用率与资源平衡](#5-占用率与资源平衡)
6. [同步优化](#6-同步优化)
7. [高级优化技术](#7-高级优化技术)

---

## 1. 性能分析基础

### 1.1 性能瓶颈识别

GPU 程序的性能主要受三个因素限制：

```
┌────────────────────────────────────────────┐
│  1. 计算受限 (Compute-Bound)               │
│     - ALU 利用率高                         │
│     - 内存带宽未饱和                       │
│     - 优化方向：减少指令、提高 ILP         │
└────────────────────────────────────────────┘

┌────────────────────────────────────────────┐
│  2. 内存受限 (Memory-Bound)                │
│     - 内存带宽饱和                         │
│     - ALU 空闲等待数据                     │
│     - 优化方向：减少内存访问、提高合并度   │
└────────────────────────────────────────────┘

┌────────────────────────────────────────────┐
│  3. 延迟受限 (Latency-Bound)               │
│     - 占用率低，无法隐藏延迟               │
│     - 优化方向：提高占用率、流水线         │
└────────────────────────────────────────────┘
```

### 1.2 性能指标

#### 关键指标定义

```
1. 吞吐量（Throughput）
   = 完成的工作量 / 时间
   单位：GFLOPS, GB/s, 元素/秒

2. 延迟（Latency）
   = 单个操作的完成时间
   单位：周期, 毫秒

3. 带宽利用率
   = 实际带宽 / 峰值带宽 × 100%

4. 计算强度（Arithmetic Intensity）
   = FLOP 数量 / 访问字节数
   单位：FLOP/Byte
```

#### Roofline 模型

```
性能分析的经典模型：
┌────────────────────────────────────────┐
│         Performance (GFLOPS)           │
│            │                           │
│   计算上限 ├──────────────────────────│ 峰值算力
│            │         /               │
│            │       /                 │
│  实际性能  │     /                   │
│            │   /                     │
│   内存上限 │ /                       │
│            └─────────────────────────│
│          Arithmetic Intensity         │
│          (FLOP/Byte)                  │
└────────────────────────────────────────┘

关键点：
- 低计算强度（< 10 FLOP/Byte）：内存受限
- 高计算强度（> 100 FLOP/Byte）：计算受限
- 中间区域：混合受限
```

**示例计算**：
```
向量加法：C[i] = A[i] + B[i]
- 操作：1 次加法 = 1 FLOP
- 内存访问：3 × 4 字节 = 12 字节（读 A、B，写 C）
- 计算强度：1 / 12 ≈ 0.08 FLOP/Byte
→ 严重内存受限！

矩阵乘法（N×N）：
- 操作：N³ 次乘加 = 2N³ FLOP
- 内存访问：3N² × 4 字节（读 A、B，写 C）
- 计算强度：2N³ / (12N²) = N / 6 FLOP/Byte
→ N=1024 时约 170 FLOP/Byte，计算受限 ✅
```

---

## 2. 内存访问优化

### 2.1 合并访存（Coalesced Access）详解

#### 硬件层面的内存事务

```
GPU 内存事务机制（L1 关闭时）：
┌────────────────────────────────────────┐
│  L2 Cache 事务粒度：32 字节            │
├────────────────────────────────────────┤
│  Warp 访问会被分组为多个事务：         │
│  - 每个事务获取 32 字节对齐的数据      │
│  - 最优：1 个 Warp → 1-4 个事务       │
│  - 最坏：1 个 Warp → 32 个事务        │
└────────────────────────────────────────┘
```

#### 完美合并的条件

```
理想访问模式（4 字节元素）：
┌────────────────────────────────────────┐
│ Thread 0:  data[base + 0]  ────┐      │
│ Thread 1:  data[base + 1]      │      │
│ Thread 2:  data[base + 2]      │      │  连续
│ ...                             ├─ 128B │  对齐
│ Thread 31: data[base + 31] ────┘      │  = 1 个事务
└────────────────────────────────────────┘

条件：
1. 连续访问（stride = 1）
2. 起始地址对齐（base % 128 == 0）
3. 访问在一个 Cache Line 内
```

#### 非合并访问的性能损失

```
案例分析：跨步访问

// stride = 2
float val = data[tid * 2];

Warp 访问地址：
  Thread 0:  0x0000 ─┐
  Thread 1:  0x0008  │ 128B
  ...               │
  Thread 15: 0x0078 ─┘
  Thread 16: 0x0080 ─┐
  ...               │ 128B
  Thread 31: 0x00F8 ─┘

结果：2 个 128B 事务，但只用了 50% 数据
带宽利用率：50%

// stride = 32（最坏情况）
float val = data[tid * 32];

每个线程访问不同的 Cache Line
结果：32 个独立事务
带宽利用率：~3%（1/32）
```

#### 优化策略

**策略 1：转置访问模式**
```cuda
// ❌ 列主序读取（非合并）
for (int i = 0; i < N; i++) {
    sum += matrix[threadIdx.x + i * width];  // 跨步访问
}

// ✅ 行主序读取（合并）
for (int i = 0; i < N; i++) {
    sum += matrix[i + threadIdx.x * width];  // 连续访问
}
```

**策略 2：使用共享内存缓冲**
```cuda
// 矩阵转置：通过共享内存改变访问模式
__shared__ float tile[TILE_SIZE][TILE_SIZE];

// 合并读取
tile[threadIdx.y][threadIdx.x] = input[...];
__syncthreads();

// 合并写入（转置后）
output[...] = tile[threadIdx.x][threadIdx.y];
```

**策略 3：向量化加载**
```cuda
// ❌ 逐个加载（4 次事务）
float a = data[tid];
float b = data[tid + 1];
float c = data[tid + 2];
float d = data[tid + 3];

// ✅ 向量化加载（1 次事务）
float4 val = reinterpret_cast<float4*>(data)[tid];
float a = val.x;
float b = val.y;
float c = val.z;
float d = val.w;
```

---

### 2.2 Cache 行为与优化

#### L1/L2 Cache 架构

```
现代 GPU Cache 层次（Ampere）：
┌────────────────────────────────────────┐
│  L1 Cache / 共享内存（可配置）         │
├────────────────────────────────────────┤
│  - 每个 SM 独立                        │
│  - 容量：0-128 KB（与共享内存互斥）    │
│  - Cache Line：128 字节                │
│  - 延迟：~28 周期                      │
└────────────────────────────────────────┘

┌────────────────────────────────────────┐
│  L2 Cache（全局共享）                  │
├────────────────────────────────────────┤
│  - 所有 SM 共享                        │
│  - 容量：40 MB (A100)                  │
│  - Cache Line：128 字节                │
│  - 延迟：~200 周期                     │
└────────────────────────────────────────┘
```

#### Cache 命中与访问模式

```
时间局部性（Temporal Locality）：
  同一数据被多次访问
  ┌──────────────────────────┐
  │  for (int i = 0; i < 100; i++) {
  │      sum += data[idx];    │  重复访问同一地址
  │  }                        │  → 首次 Miss，后续 Hit
  └──────────────────────────┘

空间局部性（Spatial Locality）：
  访问连续的数据
  ┌──────────────────────────┐
  │  for (int i = 0; i < N; i++) {
  │      sum += data[i];      │  连续访问
  │  }                        │  → Cache Line 预取
  └──────────────────────────┘
```

#### L1 Cache 配置

```cuda
// 控制 L1 Cache 和共享内存的大小权衡
cudaFuncSetAttribute(myKernel,
    cudaFuncAttributePreferredSharedMemoryCarveout,
    cudaSharedmemCarveoutMaxShared);  // 优先共享内存

// 或
cudaFuncAttributePreferredSharedMemoryCarveout,
    cudaSharedmemCarveoutMaxL1);      // 优先 L1 Cache
```

#### L2 持久化（Ampere+）

```cuda
// 标记数据为"持久化"，减少 L2 驱逐
cudaStream_t stream;
cudaStreamCreate(&stream);

// 设置 L2 访问策略窗口
cudaStreamAttrValue stream_attribute;
stream_attribute.accessPolicyWindow.base_ptr = data;
stream_attribute.accessPolicyWindow.num_bytes = size;
stream_attribute.accessPolicyWindow.hitRatio = 1.0;  // 100% 命中
stream_attribute.accessPolicyWindow.hitProp = cudaAccessPropertyPersisting;
stream_attribute.accessPolicyWindow.missProp = cudaAccessPropertyStreaming;

cudaStreamSetAttribute(stream,
    cudaStreamAttributeAccessPolicyWindow,
    &stream_attribute);
```

---

### 2.3 内存对齐

#### 对齐的重要性

```
未对齐访问的性能影响：
┌────────────────────────────────────────┐
│  对齐访问（128 字节边界）              │
│  [0x0000 ──────────────── 0x007F]     │  1 个事务
│   ▲                                    │
│   └─ 访问起点                          │
└────────────────────────────────────────┘

┌────────────────────────────────────────┐
│  未对齐访问（偏移 64 字节）            │
│  [0x0040 ──────────────── 0x00BF]     │  跨越 2 个
│           ▲                            │  Cache Line
│           └─ 访问起点                  │  = 2 个事务
└────────────────────────────────────────┘

性能损失：最多 2 倍带宽浪费
```

#### 确保对齐

```cuda
// ✅ cudaMalloc 自动 256 字节对齐
float *data;
cudaMalloc(&data, size);  // 保证对齐

// ⚠️ 手动分配需要检查
float *aligned_data = (float*)aligned_alloc(256, size);

// ✅ 编译时对齐
__align__(128) __device__ float shared_data[1024];
```

---

## 3. 共享内存优化

### 3.1 Bank Conflict 深入原理

#### Bank 的硬件结构

```
共享内存物理结构（32 个 Bank）：
┌──────────────────────────────────────────┐
│  每个 Bank：                              │
│  - 宽度：4 字节（32-bit）                 │
│  - 每周期可服务 1 个请求                  │
│  - 并发服务所有 Bank                      │
└──────────────────────────────────────────┘

地址映射（连续模式）：
  地址 0-3   → Bank 0
  地址 4-7   → Bank 1
  地址 8-11  → Bank 2
  ...
  地址 124-127 → Bank 31
  地址 128-131 → Bank 0  (循环)
```

#### Conflict 的本质

```
Warp 访问共享内存时：
┌────────────────────────────────────────┐
│  步骤 1：计算每个线程访问哪个 Bank    │
│  步骤 2：检测同一 Bank 的多个访问     │
│  步骤 3：冲突访问串行化               │
└────────────────────────────────────────┘

无冲突（最优）：
  32 个线程 → 32 个不同 Bank
  延迟：1 个周期

2-way Conflict：
  16 对线程 → 各访问同一 Bank
  延迟：2 个周期（串行 2 次）

32-way Conflict（最坏）：
  32 个线程 → 同一个 Bank
  延迟：32 个周期
```

#### 特殊情况：Broadcast

```
Broadcast（广播）：无惩罚的特殊情况
┌────────────────────────────────────────┐
│  所有线程访问同一个地址                │
│  → 硬件自动广播，无冲突                │
└────────────────────────────────────────┘

__shared__ float data[32];
float val = data[0];  // 所有线程读同一地址
// 硬件检测并广播，延迟 = 1 周期 ✅
```

---

### 3.2 优化技术

#### 技巧 1：Padding 消除冲突

```cuda
// ❌ 矩阵转置有冲突
__shared__ float tile[32][32];

// 线程 (i, j) 写入
tile[threadIdx.y][threadIdx.x] = input[...];  // 无冲突
__syncthreads();

// 线程 (i, j) 读取转置
output[...] = tile[threadIdx.x][threadIdx.y];  // 32-way conflict!

// 原因分析：
// Thread 0 访问 tile[0][0] → Bank 0
// Thread 1 访问 tile[0][1] → Bank 0 (Conflict!)
// ...
// Thread 31 访问 tile[0][31] → Bank 0 (Conflict!)
```

```cuda
// ✅ Padding 修复
__shared__ float tile[32][33];  // 多一列

// 新的 Bank 映射：
// tile[0][0]  → Bank 0
// tile[0][1]  → Bank 1
// ...
// tile[0][31] → Bank 31
// tile[0][32] → Bank 32 % 32 = Bank 0
// tile[1][0]  → Bank 1  (因为偏移 33 × 4 字节)
// tile[1][1]  → Bank 2
// ...

// 现在转置读取无冲突！
```

**为什么 33 有效？**
```
数学原理：
  33 和 32 互质 → 访问模式分散到不同 Bank
  通用公式：Padding = BankSize + 1
```

#### 技巧 2：改变访问模式

```cuda
// ❌ 有冲突的规约
__shared__ float sdata[256];
for (unsigned int s = 1; s < blockDim.x; s *= 2) {
    if (tid % (2 * s) == 0) {
        sdata[tid] += sdata[tid + s];
        // 访问 sdata[0], sdata[2], sdata[4], ...
        // 步长 = 2^k → Bank Conflict
    }
    __syncthreads();
}

// ✅ 无冲突的规约
for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {
    if (tid < s) {
        sdata[tid] += sdata[tid + s];
        // 访问 sdata[0], sdata[1], sdata[2], ...
        // 连续访问 → 无 Conflict
    }
    __syncthreads();
}
```

#### 技巧 3：向量化访问

```cuda
// ✅ 使用 float4 减少访问次数
__shared__ float data[1024];

// 加载到共享内存（向量化）
float4 *ptr = reinterpret_cast<float4*>(&data[threadIdx.x * 4]);
*ptr = *reinterpret_cast<float4*>(&input[globalIdx * 4]);

// 好处：
// 1. 减少访问次数（1/4）
// 2. 可能改善 Bank 访问模式
```

---

### 3.3 共享内存使用模式

#### 双缓冲（Double Buffering）

```cuda
__global__ void pipelinedKernel(float *input, float *output, int n) {
    __shared__ float buffer[2][TILE_SIZE];  // 双缓冲

    int bufIdx = 0;

    // 预加载第一块
    buffer[bufIdx][threadIdx.x] = input[blockIdx.x * TILE_SIZE + threadIdx.x];
    __syncthreads();

    for (int tile = 1; tile < numTiles; tile++) {
        // 切换缓冲区
        int nextBuf = 1 - bufIdx;

        // 并行：处理当前块 + 加载下一块
        // 计算（使用 buffer[bufIdx]）
        float result = compute(buffer[bufIdx][threadIdx.x]);

        // 同时加载下一块到 buffer[nextBuf]
        if (tile < numTiles) {
            buffer[nextBuf][threadIdx.x] = input[tile * TILE_SIZE + threadIdx.x];
        }

        // 写回结果
        output[(tile-1) * TILE_SIZE + threadIdx.x] = result;

        __syncthreads();
        bufIdx = nextBuf;
    }
}
```

**优势**：
- 隐藏全局内存延迟
- 计算和加载重叠
- 提高吞吐量

---

## 4. 指令级优化

### 4.1 指令级并行（ILP）

#### ILP 的原理

```
GPU 流水线（简化）：
┌────────────────────────────────────────┐
│  每个 CUDA Core 的流水线：             │
│  [取指] → [译码] → [执行] → [写回]    │
│    1      1       4-10     1  (周期)  │
└────────────────────────────────────────┘

ILP 利用流水线：
  周期 0: 指令 1 [取指]
  周期 1: 指令 1 [译码] | 指令 2 [取指]
  周期 2: 指令 1 [执行] | 指令 2 [译码] | 指令 3 [取指]
  ...

如果指令 1-3 无依赖 → 可以并行执行
```

#### 提高 ILP 的技术

**技术 1：循环展开（Loop Unrolling）**
```cuda
// ❌ 低 ILP（循环依赖）
for (int i = 0; i < n; i++) {
    sum += data[i];  // 每次迭代依赖上一次的 sum
}

// ✅ 循环展开（提高 ILP）
for (int i = 0; i < n; i += 4) {
    sum1 += data[i];
    sum2 += data[i+1];  // 独立计算
    sum3 += data[i+2];  // 可以并行
    sum4 += data[i+3];
}
sum = sum1 + sum2 + sum3 + sum4;

// 编译器优化：
#pragma unroll 4
for (int i = 0; i < n; i++) {
    sum += data[i];
}
```

**技术 2：独立指令重排**
```cuda
// ❌ 指令依赖链
float a = x * y;
float b = a + z;      // 依赖 a
float c = b * w;      // 依赖 b

// ✅ 减少依赖
float a = x * y;
float d = z * w;      // 独立计算
float b = a + z;
float c = b + d;
```

**技术 3：多累加器**
```cuda
// ✅ 向量点积优化
float sum1 = 0, sum2 = 0, sum3 = 0, sum4 = 0;
for (int i = 0; i < n; i += 4) {
    sum1 += a[i] * b[i];
    sum2 += a[i+1] * b[i+1];  // 4 路并行
    sum3 += a[i+2] * b[i+2];
    sum4 += a[i+3] * b[i+3];
}
return sum1 + sum2 + sum3 + sum4;
```

---

### 4.2 数学函数优化

#### 内置函数（Intrinsics）

```cuda
// 标准函数 vs 快速内置函数
┌────────────────────────────────────────┐
│  标准函数（精确但慢）                  │
│  - sqrtf()   : ~20 周期               │
│  - expf()    : ~30 周期               │
│  - logf()    : ~30 周期               │
└────────────────────────────────────────┘

┌────────────────────────────────────────┐
│  快速内置函数（近似但快）              │
│  - __fsqrt_rn(): ~2 周期  (10x 快)   │
│  - __expf():    ~5 周期   (6x 快)    │
│  - __logf():    ~5 周期   (6x 快)    │
└────────────────────────────────────────┘

使用 -use_fast_math 自动替换
```

**示例**：
```cuda
// ❌ 慢速（精确到最后一位）
float y = sqrtf(x);

// ✅ 快速（精度略降）
float y = __fsqrt_rn(x);  // rn = round to nearest

// 或编译时全局开启：
// nvcc -use_fast_math kernel.cu
```

#### 避免昂贵的操作

```cuda
// ❌ 除法慢（~20 周期）
float result = a / b;

// ✅ 改为乘法（~2 周期）
float inv_b = 1.0f / b;  // 预计算
float result = a * inv_b;

// ❌ 模运算慢
int idx = i % n;

// ✅ 位运算（如果 n 是 2 的幂）
int idx = i & (n - 1);  // 等价于 i % n，当 n = 2^k

// ❌ 分支慢（可能发散）
float result = (x > 0) ? a : b;

// ✅ 无分支（使用 fmaxf/fminf）
float result = fmaxf(x, 0.0f) * a + fminf(x, 0.0f) * b;
```

---

### 4.3 Warp Shuffle 优化

Warp Shuffle 允许 Warp 内线程直接交换数据，**无需共享内存**。

```cuda
// 传统方法（使用共享内存）
__shared__ float sdata[32];
sdata[threadIdx.x] = val;
__syncthreads();
float neighbor = sdata[threadIdx.x + 1];

// ✅ Warp Shuffle（更快）
float neighbor = __shfl_down_sync(0xffffffff, val, 1);
//                                 ▲           ▲    ▲
//                                 mask       val  offset
```

#### Warp 规约示例

```cuda
// 使用 Shuffle 的快速规约
__device__ float warpReduce(float val) {
    for (int offset = 16; offset > 0; offset >>= 1) {
        val += __shfl_down_sync(0xffffffff, val, offset);
        // offset=16: [0]+[16], [1]+[17], ...
        // offset=8:  [0]+[8],  [1]+[9],  ...
        // offset=4:  [0]+[4],  [1]+[5],  ...
        // offset=2:  [0]+[2],  [1]+[3]
        // offset=1:  [0]+[1]
    }
    return val;  // Thread 0 holds the sum
}

// 完整 Block 规约
__global__ void reduceKernel(float *input, float *output, int n) {
    __shared__ float warpSums[32];  // 最多 32 个 Warp

    int tid = threadIdx.x + blockIdx.x * blockDim.x;
    float val = (tid < n) ? input[tid] : 0.0f;

    // Warp 内规约
    val = warpReduce(val);

    // 每个 Warp 的第一个线程写入共享内存
    int warpId = threadIdx.x / 32;
    if (threadIdx.x % 32 == 0) {
        warpSums[warpId] = val;
    }
    __syncthreads();

    // 最后一个 Warp 完成最终规约
    if (threadIdx.x < 32) {
        val = (threadIdx.x < (blockDim.x + 31) / 32) ? warpSums[threadIdx.x] : 0;
        val = warpReduce(val);
    }

    if (threadIdx.x == 0) {
        output[blockIdx.x] = val;
    }
}
```

**优势**：
- 延迟：1 周期（vs 共享内存 ~20 周期）
- 无 Bank Conflict
- 不占用共享内存资源

---

## 5. 占用率与资源平衡

### 5.1 占用率的真相

#### 占用率计算

```
占用率（Occupancy）= 活跃 Warp 数 / 最大 Warp 数

示例（Ampere A100 SM）：
┌────────────────────────────────────────┐
│  硬件限制（每个 SM）：                  │
│  - 最大 Warp：64                       │
│  - 最大线程：2048                      │
│  - 最大 Block：32                      │
│  - 寄存器：65,536                      │
│  - 共享内存：164 KB                    │
└────────────────────────────────────────┘

Kernel 配置：256 线程/Block, 48 寄存器/线程, 8KB 共享内存/Block

计算限制：
1. Warp 限制：
   每 Block = 256 / 32 = 8 Warp
   最多 Block = 64 / 8 = 8 Blocks

2. 寄存器限制：
   每 Block = 256 × 48 = 12,288 寄存器
   最多 Block = 65,536 / 12,288 = 5 Blocks  ← 瓶颈！

3. 共享内存限制：
   每 Block = 8 KB
   最多 Block = 164 KB / 8 KB = 20 Blocks

4. Block 数限制：
   最多 32 Blocks（硬件）

实际：min(8, 5, 20, 32) = 5 Blocks
占用率 = (5 × 8) / 64 = 62.5%
```

---

### 5.2 占用率与性能的关系

**误区**："占用率越高越好"

**真相**：
```
┌────────────────────────────────────────┐
│  计算密集型（Compute-Bound）           │
│  - 少量内存访问，大量计算              │
│  - 延迟已被计算隐藏                    │
│  - 低占用率（30-50%）足够             │
│  - 更多寄存器/共享内存更重要           │
└────────────────────────────────────────┘

┌────────────────────────────────────────┐
│  内存密集型（Memory-Bound）            │
│  - 大量内存访问，少量计算              │
│  - 需要隐藏内存延迟                    │
│  - 高占用率（75-100%）有帮助          │
│  - 更多活跃 Warp 隐藏延迟             │
└────────────────────────────────────────┘
```

**案例研究**：
```
矩阵乘法（分块算法）：
  - 每个线程使用 64 个寄存器
  - 占用率：50%
  - 但性能接近峰值（因为计算密集）

  降低寄存器使用到 32 个：
  - 占用率：100%
  - 但性能下降（因为需要更多内存访问）
```

---

### 5.3 资源调优策略

#### 策略 1：调整 Block 大小

```
测试不同的 Block 大小：
┌────────────────────────────────────────┐
│  Block Size  │ Occupancy │ Performance │
├──────────────┼───────────┼─────────────┤
│  128         │   50%     │  ★★☆☆☆     │  太小
│  256         │   75%     │  ★★★★☆     │  平衡
│  512         │  100%     │  ★★★★★     │  最优
│  1024        │   50%     │  ★★★☆☆     │  寄存器受限
└────────────────────────────────────────┘

一般建议：128-512 之间
```

#### 策略 2：限制寄存器使用

```cuda
// 方法 1：编译时全局限制
nvcc -maxrregcount=32 kernel.cu

// 方法 2：Per-Kernel 限制
__global__ __launch_bounds__(256, 4)  // 256 线程, 最少 4 Blocks/SM
void myKernel(...) {
    // ...
}
```

#### 策略 3：动态共享内存

```cuda
// 静态分配（占用资源）
__shared__ float sdata[1024];

// 动态分配（灵活）
extern __shared__ float sdata[];

// 启动时指定大小
myKernel<<<blocks, threads, sharedMemSize>>>(args);
```

---

## 6. 同步优化

### 6.1 同步开销

```
__syncthreads() 的代价：
┌────────────────────────────────────────┐
│  1. 等待所有线程到达同步点              │
│     - 最慢线程决定同步时间              │
│  2. 刷新所有内存写入                    │
│  3. 执行屏障（Barrier）                │
│  典型开销：10-50 周期                  │
└────────────────────────────────────────┘
```

#### 避免不必要的同步

```cuda
// ❌ 过度同步
__shared__ float sdata[256];
for (int i = 0; i < 10; i++) {
    sdata[tid] = compute(i);
    __syncthreads();  // 每次迭代都同步（慢）
    float val = sdata[(tid + 1) % 256];
    __syncthreads();  // 又一次同步
}

// ✅ 减少同步
__shared__ float sdata[256];
for (int i = 0; i < 10; i++) {
    sdata[tid] = compute(i);
    __syncthreads();  // 只在需要时同步
    float val = sdata[(tid + 1) % 256];
    // 如果下一次迭代不依赖 val，无需再次同步
}
```

---

### 6.2 Warp 级同步

```cuda
// ✅ Warp 内无需 __syncthreads()
if (tid < 32) {
    // Warp 内操作自动同步（SIMT）
    sdata[tid] += sdata[tid + 16];
    sdata[tid] += sdata[tid + 8];
    sdata[tid] += sdata[tid + 4];
    sdata[tid] += sdata[tid + 2];
    sdata[tid] += sdata[tid + 1];
    // 无需 __syncthreads()
}
```

---

### 6.3 原子操作优化

```
原子操作的性能：
┌────────────────────────────────────────┐
│  问题：串行化                           │
│  atomicAdd(&counter, 1)                │
│  - 所有线程竞争同一地址                │
│  - 完全串行化（最坏情况）              │
│  - 延迟：~200 周期/操作                │
└────────────────────────────────────────┘
```

#### 优化策略：先局部后全局

```cuda
// ❌ 直接全局原子操作（慢）
__global__ void slowHistogram(int *data, int *histogram, int n) {
    int tid = threadIdx.x + blockIdx.x * blockDim.x;
    if (tid < n) {
        atomicAdd(&histogram[data[tid]], 1);  // 全局竞争
    }
}

// ✅ 两级聚合（快）
__global__ void fastHistogram(int *data, int *histogram, int n) {
    __shared__ int localHist[256];

    // 初始化
    if (threadIdx.x < 256) {
        localHist[threadIdx.x] = 0;
    }
    __syncthreads();

    // 局部累加（Block 内竞争，较少）
    int tid = threadIdx.x + blockIdx.x * blockDim.x;
    if (tid < n) {
        atomicAdd(&localHist[data[tid]], 1);
    }
    __syncthreads();

    // 合并到全局（每个 Block 只做一次）
    if (threadIdx.x < 256) {
        if (localHist[threadIdx.x] > 0) {
            atomicAdd(&histogram[threadIdx.x], localHist[threadIdx.x]);
        }
    }
}

// 性能提升：10-100 倍
```

---

## 7. 高级优化技术

### 7.1 Stream 并发

```cuda
// 多 Stream 重叠计算和传输
cudaStream_t stream[4];
for (int i = 0; i < 4; i++) {
    cudaStreamCreate(&stream[i]);
}

for (int i = 0; i < 4; i++) {
    // 异步传输
    cudaMemcpyAsync(d_data[i], h_data[i], size,
                    cudaMemcpyHostToDevice, stream[i]);

    // 异步计算
    kernel<<<blocks, threads, 0, stream[i]>>>(d_data[i], ...);

    // 异步传回
    cudaMemcpyAsync(h_result[i], d_result[i], size,
                    cudaMemcpyDeviceToHost, stream[i]);
}

// 所有 Stream 并发执行，隐藏传输延迟
```

---

### 7.2 Tensor Core 使用

```cuda
#include <mma.h>
using namespace nvcuda::wmma;

__global__ void wmmaKernel(half *A, half *B, float *C) {
    // 声明 Fragment（寄存器）
    fragment<matrix_a, 16, 16, 16, half, row_major> a_frag;
    fragment<matrix_b, 16, 16, 16, half, col_major> b_frag;
    fragment<accumulator, 16, 16, 16, float> c_frag;

    // 加载矩阵块
    load_matrix_sync(a_frag, A, 16);
    load_matrix_sync(b_frag, B, 16);
    fill_fragment(c_frag, 0.0f);

    // Tensor Core 计算：C = A × B
    mma_sync(c_frag, a_frag, b_frag, c_frag);

    // 存储结果
    store_matrix_sync(C, c_frag, 16, mem_row_major);
}

// 性能：100+ 倍于 CUDA Core（FP16）
```

---

## 总结

### 优化优先级

```
1. 算法选择                    (100x-1000x)
2. 内存访问模式（合并访存）     (10x-100x)
3. 共享内存使用                (2x-10x)
4. 占用率调优                  (1.5x-3x)
5. 指令级优化                  (1.2x-2x)
6. 数学函数优化                (1.1x-2x)
```

### 性能检查清单

- [ ] 内存访问是否合并？
- [ ] 是否避免了 Bank Conflict？
- [ ] 占用率是否合理（根据类型）？
- [ ] 是否避免了分支发散？
- [ ] 同步是否最少化？
- [ ] 是否使用了快速数学函数？
- [ ] 是否利用了 Warp Shuffle？
- [ ] 是否考虑了 Stream 并发？

### 工具推荐

- **Nsight Compute**：Kernel 级性能分析
- **Nsight Systems**：系统级时间线分析
- **compute-sanitizer**：内存错误检测
- **CUDA Occupancy Calculator**：占用率计算

---

**相关文档**：
- [CUDA 架构与原理](CUDA架构与原理.md) - 硬件架构基础
- [CUDA 学习指南](CUDA学习指南.md) - 编程入门
- [FAQ](FAQ.md) - 常见性能问题解答
